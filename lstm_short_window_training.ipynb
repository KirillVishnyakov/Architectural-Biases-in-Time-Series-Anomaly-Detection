{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":14851887,"datasetId":9499652,"databundleVersionId":15712237}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch.nn as nn\nimport torch\nimport torch.utils.data as data\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.decomposition import PCA\nimport os\nfrom itertools import product\nimport copy\nimport sys\n\n!rm -r /kaggle/working/Architectural-Biases-in-Time-Series-Anomaly-Detection\n!git clone https://github.com/KirillVishnyakov/Architectural-Biases-in-Time-Series-Anomaly-Detection","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-25T03:16:57.623149Z","iopub.execute_input":"2026-02-25T03:16:57.623760Z","iopub.status.idle":"2026-02-25T03:17:03.493907Z","shell.execute_reply.started":"2026-02-25T03:16:57.623729Z","shell.execute_reply":"2026-02-25T03:17:03.493022Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'Architectural-Biases-in-Time-Series-Anomaly-Detection'...\nremote: Enumerating objects: 65, done.\u001b[K\nremote: Counting objects: 100% (65/65), done.\u001b[K\nremote: Compressing objects: 100% (48/48), done.\u001b[K\nremote: Total 65 (delta 35), reused 42 (delta 15), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (65/65), 57.65 KiB | 831.00 KiB/s, done.\nResolving deltas: 100% (35/35), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"### Import necessary functions/modules from git repository","metadata":{}},{"cell_type":"code","source":"sys.path.append(\"/kaggle/working/Architectural-Biases-in-Time-Series-Anomaly-Detection\")\nimport train\nfrom dataset import myDataset\nfrom lstm import myLSTM\nfrom utils import load_config","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T03:17:12.424069Z","iopub.execute_input":"2026-02-25T03:17:12.425137Z","iopub.status.idle":"2026-02-25T03:17:12.450235Z","shell.execute_reply.started":"2026-02-25T03:17:12.425099Z","shell.execute_reply":"2026-02-25T03:17:12.449498Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"### load device, datasets, config into environment.","metadata":{}},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device(\"cuda\")\ntrain_dataset = myDataset(device, 5)\ntest_dataset = myDataset(device, 5, False)\nbase_config = load_config(\"/kaggle/working/Architectural-Biases-in-Time-Series-Anomaly-Detection/config.yaml\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T03:17:41.347759Z","iopub.execute_input":"2026-02-25T03:17:41.348100Z","iopub.status.idle":"2026-02-25T03:18:22.058984Z","shell.execute_reply.started":"2026-02-25T03:17:41.348073Z","shell.execute_reply":"2026-02-25T03:18:22.058159Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"### Tune short window LSTM Forecaster up to a reasonable point and save it.","metadata":{}},{"cell_type":"code","source":"num_layers_list = [1, 2]\nlr_list = [1e-3, 5e-4]\nbatch_sizes = [10, 20]\ndropouts = [0.0]\n\ntraining_results = dict()\n\nfor nl, lr, bs, dr in product(num_layers_list, lr_list, batch_sizes, dropouts):\n    config = copy.deepcopy(base_config)\n    \n    config[\"model\"][\"num_layers\"] = nl\n    config[\"model\"][\"dropout\"] = dr\n    config[\"training\"][\"lr\"] = lr\n    config[\"training\"][\"batch_size\"] = bs\n\n    config[\"training\"][\"num_epochs\"] = 50\n    \n    config[\"experiment_name\"] = f\"l{nl}_lr{lr}_bs{bs}_dr{dr}\"\n    model = myLSTM(**config[\"model\"]).to(device)\n    training_results[config[\"experiment_name\"]] = \\\n    train.fit_lstm(model, config[\"experiment_name\"], train_dataset, test_dataset, **config[\"training\"])\n    print()\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T03:18:31.736408Z","iopub.execute_input":"2026-02-25T03:18:31.736743Z","execution_failed":"2026-02-25T03:38:40.422Z"}},"outputs":[{"name":"stdout","text":"| experiment: l1_lr0.001_bs10_dr0.0 | epoch 0, train: MSE 0.0935, test MSE: 0.1469\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 1, train: MSE 0.0618, test MSE: 0.1249\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 2, train: MSE 0.0476, test MSE: 0.0921\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 3, train: MSE 0.0417, test MSE: 0.0711\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 4, train: MSE 0.0384, test MSE: 0.0616\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 5, train: MSE 0.0363, test MSE: 0.0569\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 6, train: MSE 0.0351, test MSE: 0.0550\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 7, train: MSE 0.0343, test MSE: 0.0545\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 8, train: MSE 0.0334, test MSE: 0.0532\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 9, train: MSE 0.0327, test MSE: 0.0527\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 10, train: MSE 0.0324, test MSE: 0.0528\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 11, train: MSE 0.0320, test MSE: 0.0529\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 12, train: MSE 0.0316, test MSE: 0.0523\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 13, train: MSE 0.0312, test MSE: 0.0510\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 14, train: MSE 0.0308, test MSE: 0.0493\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 15, train: MSE 0.0304, test MSE: 0.0475\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 16, train: MSE 0.0301, test MSE: 0.0462\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 17, train: MSE 0.0297, test MSE: 0.0454\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 18, train: MSE 0.0293, test MSE: 0.0450\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 19, train: MSE 0.0288, test MSE: 0.0445\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 20, train: MSE 0.0283, test MSE: 0.0439\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 21, train: MSE 0.0280, test MSE: 0.0438\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 22, train: MSE 0.0279, test MSE: 0.0440\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 23, train: MSE 0.0277, test MSE: 0.0439\nupdate LR: 0.001 -> 0.0005\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 24, train: MSE 0.0194, test MSE: 0.0310\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 25, train: MSE 0.0181, test MSE: 0.0284\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 26, train: MSE 0.0177, test MSE: 0.0278\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 27, train: MSE 0.0174, test MSE: 0.0276\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 28, train: MSE 0.0173, test MSE: 0.0275\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 29, train: MSE 0.0171, test MSE: 0.0276\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 30, train: MSE 0.0171, test MSE: 0.0277\nupdate LR: 0.0005 -> 0.00025\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 31, train: MSE 0.0144, test MSE: 0.0224\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 32, train: MSE 0.0138, test MSE: 0.0210\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 33, train: MSE 0.0135, test MSE: 0.0204\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 34, train: MSE 0.0133, test MSE: 0.0200\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 35, train: MSE 0.0132, test MSE: 0.0198\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 36, train: MSE 0.0132, test MSE: 0.0196\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 37, train: MSE 0.0131, test MSE: 0.0195\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 38, train: MSE 0.0131, test MSE: 0.0194\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 39, train: MSE 0.0130, test MSE: 0.0194\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 40, train: MSE 0.0130, test MSE: 0.0193\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 41, train: MSE 0.0130, test MSE: 0.0193\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 42, train: MSE 0.0130, test MSE: 0.0193\nupdate LR: 0.00025 -> 0.000125\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 43, train: MSE 0.0120, test MSE: 0.0177\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 44, train: MSE 0.0118, test MSE: 0.0173\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 45, train: MSE 0.0117, test MSE: 0.0169\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 46, train: MSE 0.0116, test MSE: 0.0167\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 47, train: MSE 0.0116, test MSE: 0.0164\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 48, train: MSE 0.0115, test MSE: 0.0162\n| experiment: l1_lr0.001_bs10_dr0.0 | epoch 49, train: MSE 0.0115, test MSE: 0.0161\n\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 0, train: MSE 0.0889, test MSE: 0.1283\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 1, train: MSE 0.0508, test MSE: 0.0748\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 2, train: MSE 0.0349, test MSE: 0.0499\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 3, train: MSE 0.0293, test MSE: 0.0384\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 4, train: MSE 0.0271, test MSE: 0.0325\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 5, train: MSE 0.0253, test MSE: 0.0290\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 6, train: MSE 0.0238, test MSE: 0.0281\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 7, train: MSE 0.0226, test MSE: 0.0279\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 8, train: MSE 0.0219, test MSE: 0.0281\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 9, train: MSE 0.0215, test MSE: 0.0287\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 10, train: MSE 0.0213, test MSE: 0.0296\nupdate LR: 0.001 -> 0.0005\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 11, train: MSE 0.0164, test MSE: 0.0232\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 12, train: MSE 0.0156, test MSE: 0.0209\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 13, train: MSE 0.0153, test MSE: 0.0203\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 14, train: MSE 0.0150, test MSE: 0.0200\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 15, train: MSE 0.0148, test MSE: 0.0198\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 16, train: MSE 0.0146, test MSE: 0.0197\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 17, train: MSE 0.0144, test MSE: 0.0196\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 18, train: MSE 0.0143, test MSE: 0.0195\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 19, train: MSE 0.0142, test MSE: 0.0195\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 20, train: MSE 0.0141, test MSE: 0.0196\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 21, train: MSE 0.0141, test MSE: 0.0196\nupdate LR: 0.0005 -> 0.00025\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 22, train: MSE 0.0122, test MSE: 0.0167\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 23, train: MSE 0.0120, test MSE: 0.0165\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 24, train: MSE 0.0119, test MSE: 0.0165\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 25, train: MSE 0.0118, test MSE: 0.0164\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 26, train: MSE 0.0118, test MSE: 0.0164\nupdate LR: 0.00025 -> 0.000125\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 27, train: MSE 0.0113, test MSE: 0.0157\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 28, train: MSE 0.0112, test MSE: 0.0154\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 29, train: MSE 0.0111, test MSE: 0.0153\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 30, train: MSE 0.0111, test MSE: 0.0152\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 31, train: MSE 0.0111, test MSE: 0.0152\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 32, train: MSE 0.0111, test MSE: 0.0151\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 33, train: MSE 0.0111, test MSE: 0.0151\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 34, train: MSE 0.0111, test MSE: 0.0151\nupdate LR: 0.000125 -> 6.25e-05\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 35, train: MSE 0.0109, test MSE: 0.0151\nupdate LR: 6.25e-05 -> 3.125e-05\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 36, train: MSE 0.0108, test MSE: 0.0151\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 37, train: MSE 0.0108, test MSE: 0.0150\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 38, train: MSE 0.0108, test MSE: 0.0149\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 39, train: MSE 0.0108, test MSE: 0.0149\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 40, train: MSE 0.0108, test MSE: 0.0149\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 41, train: MSE 0.0108, test MSE: 0.0148\nupdate LR: 3.125e-05 -> 1.5625e-05\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 42, train: MSE 0.0108, test MSE: 0.0148\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 43, train: MSE 0.0108, test MSE: 0.0148\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 44, train: MSE 0.0108, test MSE: 0.0148\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 45, train: MSE 0.0108, test MSE: 0.0148\nupdate LR: 1.5625e-05 -> 7.8125e-06\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 46, train: MSE 0.0108, test MSE: 0.0148\nupdate LR: 7.8125e-06 -> 3.90625e-06\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 47, train: MSE 0.0107, test MSE: 0.0148\nupdate LR: 3.90625e-06 -> 1.953125e-06\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 48, train: MSE 0.0107, test MSE: 0.0147\nupdate LR: 1.953125e-06 -> 9.765625e-07\n| experiment: l1_lr0.001_bs20_dr0.0 | epoch 49, train: MSE 0.0107, test MSE: 0.0147\n\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"## Analyzing model outputs","metadata":{}},{"cell_type":"code","source":"model.load_state_dict(training_results['l1_lr0.001_bs10_dr0.1'][0])\nwith torch.no_grad():\n    random_idx = 1000\n    random_pred = model(test_dataset[random_idx][0].unsqueeze(dim = 0))\n    real = test_dataset[random_idx][1]\nplt.plot(random_pred.cpu().detach().numpy()[0], label = \"predicted time step\")\nplt.plot(real.cpu().detach().numpy(), c = \"red\", label = \"true timestep\")\nplt.xlabel(\"feature\")\nplt.legend()\nplt.title(\"Single sample features comparison between an arbitrary predicted and true sample\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-21T07:28:48.449526Z","iopub.execute_input":"2026-02-21T07:28:48.450113Z","iopub.status.idle":"2026-02-21T07:28:48.601469Z","shell.execute_reply.started":"2026-02-21T07:28:48.450084Z","shell.execute_reply":"2026-02-21T07:28:48.600780Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Now for all data points, requires dimensionality reduction. Choosing 3d to preserve the most variance possible.","metadata":{}},{"cell_type":"code","source":"\"\"\"y_true_array = test_dataset.y.cpu().detach().numpy()\ny_pred_array = model(test_dataset.X).cpu().detach().numpy()\npca = PCA(n_components = 3)\ntrue_3d = pca.fit_transform(y_true_array)\npred_3d = pca.transform(y_pred_array)\nprint(pca.explained_variance_ratio_.sum())\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"step = 200\nfig = plt.figure(figsize=(15, 10))\nax = fig.add_subplot(111, projection='3d') \n\nax.plot(true_3d[::step,0], true_3d[::step,1], true_3d[::step,2], c='r', linewidth=0.8, label = \"true y\")\nax.plot(pred_3d[::step,0], pred_3d[::step,1], pred_3d[::step,2], c='b', linewidth=0.8, linestyle = 'dashed', label = \"predicted y\")\n\nax.set_xlabel('X Label')\nax.set_ylabel('Y Label')\nax.set_zlabel('Z Label')\n\nplt.title('reduced true features vs reduced predicted features 3D')\nplt.legend()\nplt.show()\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}